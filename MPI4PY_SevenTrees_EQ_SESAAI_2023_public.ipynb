{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7005e79f",
   "metadata": {},
   "source": [
    "# Processing DAS data using mpi4py: Seven Trees Aftershock\n",
    "## SEP: June  2023\n",
    "### Thomas Cullison, 1st Year Geophysics \n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb761d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Host-side (serial thread)\n",
    "import datetime\n",
    "import numba\n",
    "import ipyparallel\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from time import time\n",
    "from os import cpu_count\n",
    "\n",
    "start_time = time() #start timing of processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4173be67",
   "metadata": {},
   "source": [
    "## Start MPI Local Cluster: (All 30 Threads)\n",
    "\n",
    "**This is a bit different then for the Serial and DASK notebooks. We need to start cluster before we use the \"parallel\" magic.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64464b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ncore = cpu_count()\n",
    "\n",
    "# attach to a running cluster\n",
    "cluster = ipyparallel.Client(profile='mpi',n=ncore)\n",
    "print('profile:', cluster.profile)\n",
    "print(\"IDs:\", list(cluster.ids)) # Print process id numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb14fe15",
   "metadata": {},
   "source": [
    "**We also need to import packages for the parallel environment/processes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f48f0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px \n",
    "# ^-- Look!  (ipyparallel magic)\n",
    "\n",
    "\n",
    "# MPI process-side imports\n",
    "import io\n",
    "import datetime\n",
    "import h5py\n",
    "import numba\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from mpi4py import MPI     # <-- Look!\n",
    "from scipy import signal\n",
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4445ead8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "# ^-- Look again!\n",
    "\n",
    "\n",
    "# init\n",
    "# MPI.Init(): this is done by they ipyparallel.Client() part. DO NOT call MPI.Init with parallel magic.\n",
    "\n",
    "# get WORLD_COMM info\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "if rank == 0:\n",
    "    print(f'root: num-process: {comm.size}')\n",
    "    \n",
    "print(f'my rank: {rank}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92319cf",
   "metadata": {},
   "source": [
    "## Get Data from the Cloud: Function Defs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6928f74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px \n",
    "\n",
    "def gcs_download_to_local_disk(pargs,bucket=None,lpath=None):\n",
    "    \"\"\"\n",
    "       Function name explains it all\n",
    "    \"\"\"\n",
    "    fname, i = pargs\n",
    "    client = storage.Client()\n",
    "    bucket = client.get_bucket(bucket)\n",
    "    blobname = fname + '_min' + str(i+1).zfill(2) + '.npz' \n",
    "    blob = bucket.get_blob(blobname)\n",
    "    dfname = path+fname + '_min' + str(i+1).zfill(2) + '.npz' \n",
    "    blob.download_to_filename(dfname)\n",
    "    \n",
    "    \n",
    "def parallel_load_npz(pargs,path=None):\n",
    "    \"\"\"\n",
    "       Load DAS data from local disk\n",
    "    \"\"\"\n",
    "    fname, i = pargs\n",
    "    \n",
    "    dfname = path+fname + '_min' + str(i+1).zfill(2) + '.npz' \n",
    "    dt_data = np.load(dfname)\n",
    "    \n",
    "    return dt_data['data'], dt_data['time']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8069abaa",
   "metadata": {},
   "source": [
    "## Setup List of Files to Read: (a priori Knowledge Req.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9880262e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%px\n",
    "\n",
    "buckname = 'sep-allow-others' #kind of like the head/main directory -- Leave ALONE\n",
    "\n",
    "nfiles = 10 # Leave this ALONE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf74c6c",
   "metadata": {},
   "source": [
    "## Read All Files to Array -- Map to Threads : (Memory in Cluster)\n",
    "\n",
    "#### First we need to setup MPI Comm's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befe6663",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "\n",
    "# Assign a \"Color\": those who read files (ranks 0-9), and those who don't\n",
    "key = rank\n",
    "color = 0\n",
    "if nfiles-1 < rank:\n",
    "    color = 1\n",
    "file_comm = comm.Split(color,key)\n",
    "\n",
    "if nfiles-1 < rank:\n",
    "    file_comm = MPI.COMM_NULL  # dummy comm\n",
    "    \n",
    "# init comm ranks and size    \n",
    "fc_rank = -1\n",
    "fc_size = -1\n",
    "\n",
    "#get comm info if reading file\n",
    "if file_comm != MPI.COMM_NULL:\n",
    "    fc_rank = file_comm.Get_rank()\n",
    "    fc_size = file_comm.Get_size()\n",
    "    print('FILE_COMM')\n",
    "    print(f'\\nw-comm:  rank,   size    = {rank},{size}\\nfc-comm: fc_rank,fc_size = {fc_rank},{fc_size}')\n",
    "    \n",
    "comm.Barrier() #pedantic WORLD_COMM\n",
    "    \n",
    "if file_comm == MPI.COMM_NULL:\n",
    "    print('NULL_COMM')\n",
    "    print(f'\\nw-comm:  rank,   size    = {rank},{size}\\nfc-comm: fc_rank,fc_size = {fc_rank},{fc_size}')\n",
    "    \n",
    "comm.Barrier() #pedantic WORLD_COMM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f08f3a",
   "metadata": {},
   "source": [
    "### Now, participating MPI-processes get the data from the cloud and save to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7478cee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "\n",
    "\n",
    "path = './data/test/' # put YOUR local path here\n",
    "fname = 'oct_7trees_aftershock_das' # leading prefix. leave this ALONE \n",
    "\n",
    "pargs = (fname,rank)\n",
    "\n",
    "if file_comm != MPI.COMM_NULL: \n",
    "    gcs_download_to_local_disk(pargs,bucket=buckname,lpath=path)\n",
    "    \n",
    "comm.Barrier()\n",
    "\n",
    "\n",
    "# Watch Below!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1c2dd2",
   "metadata": {},
   "source": [
    "### Read compressed numpy arrays into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa33aaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "\n",
    "\n",
    "rdata = None  #non-participating ranks stay as \"None\"\n",
    "rtime = None\n",
    "\n",
    "# pargs: same as above\n",
    "\n",
    "if file_comm != MPI.COMM_NULL: \n",
    "    rdata, rtime = parallel_load_npz(pargs,path=path)\n",
    "    print(f'rank-{rank}, rdata.shape: {rdata.shape}')\n",
    "    print(f'rank-{rank}, rdata.dtype: {rdata.dtype}')\n",
    "    \n",
    "comm.Barrier()\n",
    "\n",
    "\n",
    "# Watch Below!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e823a2",
   "metadata": {},
   "source": [
    "## Gather Arrays to ROOT Process (scatterV might be more prudent) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a6f2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "\n",
    "\n",
    "gath_rdata = None\n",
    "gath_rtime = None\n",
    "\n",
    "if rank == 0:\n",
    "    gath_rdata = np.empty((nfiles, rdata.shape[0], rdata.shape[1]), dtype=rdata.dtype)\n",
    "    gath_rtime = np.empty((nfiles, rtime.shape[0]), dtype=rtime.dtype)\n",
    "\n",
    "if file_comm != MPI.COMM_NULL: \n",
    "    file_comm.Gather(rdata,gath_rdata,root=0)\n",
    "    file_comm.Gather(rtime,gath_rtime,root=0)\n",
    "    \n",
    "comm.Barrier()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99c122b",
   "metadata": {},
   "source": [
    "## Clean-up Cluster Memory (data on 10 participating ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940d6e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "\n",
    "\n",
    "if file_comm != MPI.COMM_NULL: \n",
    "    del rdata\n",
    "    rdata = None\n",
    "    del rtime\n",
    "    rtime = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b5f7c0",
   "metadata": {},
   "source": [
    "## Scale-down Cluster to One Thread: (Not the Same as Notebook Thread)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3d91baf2",
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "client.cluster.scale(1)\n",
    "client.who_has()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a49b1c6",
   "metadata": {},
   "source": [
    "## Concatenate Arrays Over Time Axis: (Notebook Thread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfd1053",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%px\n",
    "\n",
    "\n",
    "cc_rdata = None\n",
    "cc_rtime = None\n",
    "if rank == 0:\n",
    "    cc_rdata = np.hstack(gath_rdata)\n",
    "    cc_rtime = np.hstack(gath_rtime) #UTC times hsed for plotting, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015d677e",
   "metadata": {},
   "source": [
    "## Begin Processing: Function Defs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3aa195a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOOK!  (no magic, will use numba from Host/Notebook process)\n",
    "\n",
    "@numba.njit(cache=True, fastmath=True, nogil=True, parallel=True)\n",
    "def remove_median_xchannel(orig_tr):\n",
    "    rmed_traces = orig_tr.copy()\n",
    "    for it in numba.prange(orig_tr.shape[-1]):\n",
    "        rmed_traces[:, it] -= np.median(orig_tr[:, it])\n",
    "    return rmed_traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13693b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "# ^-- LOOK! (these tasks done by MPI processes)\n",
    "\n",
    "# CANNOT jit ANY of the functions below\n",
    "\n",
    "def detrend_all_traces(orig_tr):\n",
    "    det_traces = orig_tr.copy()\n",
    "    for i in range(det_traces.shape[0]): # did this because scipy mem managment is not good enough\n",
    "        signal.detrend(det_traces[i],type='constant',overwrite_data=True)\n",
    "        signal.detrend(det_traces[i],type='linear',overwrite_data=True)\n",
    "    return det_traces\n",
    "\n",
    "\n",
    "def detrend_single_trace(orig_tr):\n",
    "    det_const = signal.detrend(orig_tr,type='constant')\n",
    "    det_trace = signal.detrend(det_const,type='linear')\n",
    "    del det_const\n",
    "    return det_trace\n",
    "\n",
    "\n",
    "    \n",
    "def bandpass_butter_single_trace(trace, fs=None, b0=None, bN=None, order=5):\n",
    "    sos = signal.butter(order, (b0,bN), 'bandpass', fs=fs, output='sos')\n",
    "    bp_trace = signal.sosfiltfilt(sos, trace)\n",
    "    return bp_trace\n",
    "\n",
    "\n",
    "\n",
    "def bandpass_butter_all_traces(orig_tr, fs=None, b0=None, bN=None, order=5):\n",
    "    bp_traces = np.zeros_like(orig_tr)\n",
    "    sos = signal.butter(order, (b0,bN), 'bandpass', fs=fs, output='sos')\n",
    "    for i in range(len(orig_tr)):\n",
    "        bp_traces[i,:] = signal.sosfiltfilt(sos, orig_tr[i])\n",
    "    return bp_traces\n",
    "\n",
    "\n",
    "\n",
    "def silly_decimate_single_trace(orig_tr,q=2):\n",
    "    return orig_tr[::q]\n",
    "    #return orig_tr[::q].copy() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e727cfb2",
   "metadata": {},
   "source": [
    "## Scale-up Cluster for Data Processing: Not Used with MPI"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7df75d84",
   "metadata": {},
   "source": [
    "%%time\n",
    "client.cluster.scale(ncore)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f7b625",
   "metadata": {},
   "source": [
    "## Scatter Concatenated Data to All Cores (from Notebook to Cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db8a157",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "\n",
    "\n",
    "# Setup metadata that ONLY root has. Needed for scattering data\n",
    "d_shape = None\n",
    "buf_dtype = None\n",
    "if rank == 0:\n",
    "    d_shape = (cc_rdata.shape[0]//size,cc_rdata.shape[1]) #ndarray.shape\n",
    "    buf_dtype = cc_rdata.dtype  #float type\n",
    "    \n",
    "    \n",
    "# Broadcast Meta data\n",
    "d_shape = comm.bcast(d_shape,root=0)\n",
    "buf_dtype = comm.bcast(buf_dtype,root=0)\n",
    "\n",
    "\n",
    "# Setup receiving buffers data\n",
    "sendbuf = None  \n",
    "sc_rdata = np.zeros(d_shape,dtype=buf_dtype)\n",
    "\n",
    "\n",
    "# Only root sends data\n",
    "if rank == 0:\n",
    "    sendbuf = cc_rdata.reshape((size,d_shape[0],d_shape[1])) #IMPORTANT: reshape NO splitting\n",
    "\n",
    "comm.Barrier()\n",
    "    \n",
    "# Scatter the data --> all ranks get their respective chunks as ndarrays\n",
    "comm.Scatter(sendbuf,sc_rdata,root=0)\n",
    "\n",
    "if rank == 0:\n",
    "    del cc_rdata\n",
    "    cc_rdata = None\n",
    "    sendbuf = None\n",
    "\n",
    "comm.Barrier()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35859031",
   "metadata": {},
   "source": [
    "## Detrend Data Per Channel: CHUNKS of Channels Per MPI Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87974912",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "\n",
    "#FIXME: \"_traces\"\n",
    "det_data = detrend_all_traces(sc_rdata)\n",
    "\n",
    "del sc_rdata  #each rank cleans up their data\n",
    "\n",
    "\n",
    "# Watch Below!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc12b3d",
   "metadata": {},
   "source": [
    "## Bandpass Filter Per Channel: (Same as for Detrend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44105b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "\n",
    "\n",
    "bl = 0.025\n",
    "br = 5.0\n",
    "fs = 200\n",
    "\n",
    "bp_data = bandpass_butter_all_traces(det_data,fs=fs,b0=bl,bN=br)\n",
    "\n",
    "del det_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ee115f",
   "metadata": {},
   "source": [
    "## Decimate Per Channel: (Same as Bandpass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938a42e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "\n",
    "\n",
    "#FIXME: global before MPI Init?\n",
    "ss = 4\n",
    "dec_bp_data = bp_data[:,::ss].copy()\n",
    "\n",
    "del bp_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c362aef",
   "metadata": {},
   "source": [
    "## Gather Processed Data: From cluster to Notebook (work-around)\n",
    "\n",
    "#### First, gather data to root, then we will copy to disk so that the \"Host/Notebook\" thread can read (this is a \"poor-man's\" work-around)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e349089",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "\n",
    "\n",
    "# All processes need the variable\n",
    "g_dec_data = None\n",
    "\n",
    "\n",
    "# But, ONLY root gets this data (initalize ndarray)\n",
    "if rank == 0:\n",
    "    g_dec_data = np.empty((size, dec_bp_data.shape[0], dec_bp_data.shape[1]), dtype=dec_bp_data.dtype)\n",
    "\n",
    "    \n",
    "# Gather data to root\n",
    "comm.Gather(dec_bp_data,g_dec_data,root=0)\n",
    "\n",
    "\n",
    "# Wait for gather to finish\n",
    "comm.Barrier()\n",
    "\n",
    "\n",
    "# Reshape (new VIEW)\n",
    "if rank == 0:\n",
    "    g_dec_data = np.vstack(g_dec_data)\n",
    "    \n",
    "\n",
    "comm.Barrier()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d62ff22",
   "metadata": {},
   "source": [
    "#### Next, have root write the gathered data to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a927c7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "\n",
    "# Set file mode\n",
    "amode = MPI.MODE_WRONLY|MPI.MODE_CREATE\n",
    "\n",
    "\n",
    "# Create file descriptors (all ranks)\n",
    "f_d = MPI.File.Open(comm, './data/processed_traces.np', amode)\n",
    "f_t = MPI.File.Open(comm, './data/times_4_traces.np', amode)\n",
    "\n",
    "\n",
    "# Only root writes file\n",
    "if rank == 0:\n",
    "    wbuf_d = g_dec_data.flatten()\n",
    "    wbuf_t = cc_rtime.flatten() ## Remember this from above? It has our UTC times\n",
    "    f_d.Write(wbuf_d)\n",
    "    f_t.Write(wbuf_t)\n",
    "    \n",
    "\n",
    "# Wait for write to finish\n",
    "comm.Barrier()\n",
    "\n",
    "# close file descriptors (all ranks)\n",
    "f_d.Close()\n",
    "f_t.Close()\n",
    "\n",
    "\n",
    "# clean up data\n",
    "del g_dec_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c660be",
   "metadata": {},
   "source": [
    "## Release Cluster: (and All Related Resources, i.e. Memory, Cores, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a421936",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "\n",
    "MPI.Finalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a25614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Host/Notebook\n",
    "\n",
    "cluster.shutdown()\n",
    "cluster.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4c8fa3",
   "metadata": {},
   "source": [
    "## X-Channel Median Removal Per Time-Sample: Numba Parallel (shared memory)\n",
    "\n",
    "#### We need to first read the MPI processed data from disk (Serial). \n",
    "**We might get better performance by using MPI_All_to_All() to avoid this workaround by transposing the data within the MPI ranks. Another alternative is to communicate through an ipyparallel client-view (still trying to figure this out)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33647de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "traces_from_disk = np.fromfile('./data/processed_traces.np', dtype=np.float32)\n",
    "tdata = np.fromfile('./data/times_4_traces.np', dtype=np.int64)\n",
    "dec_bp_data = traces_from_disk.reshape((48000,30000)) # 30,000 = 120,000/ss above (sorta cheating)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c974883",
   "metadata": {},
   "source": [
    "#### Now, we remove the xchannel median (Parallel). Note, some of the runtime is due to the JIT compile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff88068",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "proc_data = remove_median_xchannel(dec_bp_data) #func() defined above in a \"Host\" cell not \"ipyparallel\" cell\n",
    "\n",
    "del dec_bp_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf2912e",
   "metadata": {},
   "source": [
    "## Total Processing Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b153714a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Done Processing')\n",
    "runtime = time() - start_time #start is in first cell\n",
    "print(f'runtime: {datetime.timedelta(seconds=runtime)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbfc826",
   "metadata": {},
   "source": [
    "## Define function for plotting: (Serial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5d9ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_seven_trees_data(data,times,pclip=.95,fig_size=(9,10)):\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    eqdate = datetime.datetime.utcfromtimestamp(times[0]//1000000)\n",
    "    start_c = 23000\n",
    "    end_c = 35000 \n",
    "    bounds = (0,nfiles*60,end_c,start_c)\n",
    "\n",
    "\n",
    "    vclip = (1-pclip)*np.abs(data[start_c:end_c+1,:]).max()\n",
    "\n",
    "\n",
    "    plt.figure(figsize=fig_size)\n",
    "    plt.imshow(data[start_c:end_c+1,:], aspect='auto', interpolation='none', cmap='gray', vmin=-vclip, vmax=vclip, extent=bounds)\n",
    "    plt.title('DAS for Seven Trees 1st-Aftershock, 3.1 EQ @' + str(eqdate) )\n",
    "    plt.xlabel('seconds from: ' + str(eqdate.time()))\n",
    "    plt.ylabel('channel')\n",
    "    \n",
    "    return plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285e0ed0",
   "metadata": {},
   "source": [
    "## 2D Plot of the Processed DAS Data: (Serial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e8e4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small Hack for Plotting (var is created inside MPI cluster)\n",
    "nfiles = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a594f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "pclip = .99\n",
    "print(f'pclip: {pclip}')\n",
    "\n",
    "plt = plot_seven_trees_data(proc_data,tdata,pclip=pclip)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6dbd5ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
