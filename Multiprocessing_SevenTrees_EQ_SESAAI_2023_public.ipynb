{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7005e79f",
   "metadata": {},
   "source": [
    "# Processing DAS data using Multiprocessing: Seven Trees Aftershock\n",
    "## SEP: June  2023\n",
    "### Thomas Cullison, 1st Year Geophysics \n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb761d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import datetime\n",
    "import h5py\n",
    "import multiprocess # <-- Look! (extension of multiprocessing package; confusing? indeed.)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from scipy import signal\n",
    "from google.cloud import storage\n",
    "from time import time\n",
    "from os import cpu_count\n",
    "from ctypes import c_float, c_int64  #used for delcaring shared memory\n",
    "\n",
    "start_time = time() #start timing of processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92319cf",
   "metadata": {},
   "source": [
    "## Get Data from the Cloud: Function Defs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6928f74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gcs_download_to_local_disk(bucket,lpath,fname,i):\n",
    "    \"\"\"\n",
    "       Function name explains it all\n",
    "    \"\"\"\n",
    "    client = storage.Client()\n",
    "    bucket = client.get_bucket(bucket)\n",
    "    blobname = fname + '_min' + str(i+1).zfill(2) + '.npz' \n",
    "    blob = bucket.get_blob(blobname)\n",
    "    dfname = path+fname + '_min' + str(i+1).zfill(2) + '.npz' \n",
    "    print(f'downloading to: {dfname}\\n')\n",
    "    blob.download_to_filename(dfname)\n",
    "    \n",
    "    \n",
    "def parallel_load_npz(path,fname,i):\n",
    "    \"\"\"\n",
    "       Load DAS data from local disk\n",
    "    \"\"\"\n",
    "    global s_rdata\n",
    "    global s_tdata\n",
    "    \n",
    "    dfname = path+fname + '_min' + str(i+1).zfill(2) + '.npz' \n",
    "    print(f'reading: {dfname}\\n')\n",
    "    dt_data = np.load(dfname)\n",
    "    s_rdata[:,i,:] = dt_data['data'][:,:]\n",
    "    s_tdata[i,:] = dt_data['time'][:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8069abaa",
   "metadata": {},
   "source": [
    "## Setup List of Files to Read: (a priori Knowledge Req.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9880262e",
   "metadata": {},
   "outputs": [],
   "source": [
    "buckname = 'sep-allow-others' #kind of like the head/main directory -- Leave ALONE\n",
    "\n",
    "nfiles = 10 # Leave this ALONE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fc92c8",
   "metadata": {},
   "source": [
    "## Start Dask Distributed Cluster: (10 Threads at Most, One-per-file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0019d3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ncore = cpu_count() #Only this part is needed for Thread Pool (later)\n",
    "print(f'ncore: {ncore}')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1a8313de",
   "metadata": {},
   "source": [
    "nwork = min(nfiles,ncore) \n",
    "\n",
    "client = Client(n_workers=nwork,processes=True,threads_per_worker=1)\n",
    "#                   Be sure this is set ----^\n",
    "\n",
    "# show dash board link\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf74c6c",
   "metadata": {},
   "source": [
    "## Read All Files to Array -- Map to Threads : (Memory in Processes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d017155a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "nchan = 48_000 #fancy commas --> underscore for python\n",
    "ntsmp = 12_000\n",
    "\n",
    "# Declare buffers for shared memory between processes\n",
    "buf_rdata = multiprocess.RawArray(c_float,nchan*ntsmp*nfiles)\n",
    "buf_tdata = multiprocess.RawArray(c_int64,ntsmp*nfiles)\n",
    "\n",
    "# \"Wrap\" above into ndarrays\n",
    "s_rdata = np.frombuffer(buf_rdata,dtype=np.float32).reshape((nchan, nfiles, ntsmp))\n",
    "s_tdata = np.frombuffer(buf_tdata,dtype=np.int64).reshape((nfiles,ntsmp))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58562d85",
   "metadata": {},
   "source": [
    "### Download files to local disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c4e047",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "path = './data/test/' # put YOUR local path here\n",
    "fname = 'oct_7trees_aftershock_das' # leading prefix. leave this ALONE \n",
    "\n",
    "# setup a list of tuples as args to mapped function\n",
    "blist = [buckname for i in range(nfiles)] # <-- Note \"nfiles\" not \"ncore\"\n",
    "lpath = [path for i in range(nfiles)] \n",
    "lfname = [fname for i in range(nfiles)] \n",
    "lidxs = [i for i in range(nfiles)] \n",
    "pargs = list(zip(blist,lpath,lfname,lidxs))\n",
    "\n",
    "# Parallel part\n",
    "with multiprocess.Pool(processes=nfiles) as pool:\n",
    "    pool.starmap(gcs_download_to_local_disk, pargs) \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82996ee",
   "metadata": {},
   "source": [
    "### Read compressed numpy arrays into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f3da47",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "# lpath:  Same as above\n",
    "# lfnam:  Same as above\n",
    "# lidx:   Same as above\n",
    "pargs = list(zip(lpath,lfname,lidxs))\n",
    "\n",
    "# Parallel part\n",
    "with multiprocess.Pool(processes=nfiles) as pool:\n",
    "    pool.starmap(parallel_load_npz, pargs) # Will copy memory (via return) to Host/Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e823a2",
   "metadata": {},
   "source": [
    "## Gather Arrays to Notebook"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3b3bfeee",
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "gathered_data = client.gather(arrs,direct=True)\n",
    "client.who_has()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99c122b",
   "metadata": {},
   "source": [
    "## Clean-up Cluster Memory"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2de2d422",
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "for t in arrs:\n",
    "    client.cancel(t)\n",
    "client.cancel(arrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b5f7c0",
   "metadata": {},
   "source": [
    "## Scale-down Cluster to One Thread: (Not the Same as Notebook Thread)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2d91d229",
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "client.cluster.scale(1)\n",
    "client.who_has()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a49b1c6",
   "metadata": {},
   "source": [
    "## Concatenate Arrays Over Time Axis: (Notebook Thread)\n",
    "\n",
    "**We only need to \"reshape\" the arrays here. Note, it's important to appropriately declare the correct shape and use the correct indicies above when we fetched the data for each file from the cloud.**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "af4cd84a",
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "tup_list = list(map(list, zip(*results)))\n",
    "rdlist = tup_list[0]\n",
    "rtlist = tup_list[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea96143",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Declare buffers shared memory of the output of bandpass filter\n",
    "tbuf_rdata = multiprocess.RawArray(c_float,nchan*ntsmp*nfiles)\n",
    "tbuf_tdata = multiprocess.RawArray(c_int64,ntsmp*nfiles)\n",
    "test_buf = multiprocess.RawArray(c_float,nchan*ntsmp*nfiles)\n",
    "\n",
    "# \"Wrap\" above into ndarrays\n",
    "cc_rdata = np.frombuffer(tbuf_rdata,dtype=np.float32).reshape((nchan, nfiles*ntsmp))\n",
    "cc_tdata = np.frombuffer(tbuf_tdata,dtype=np.int64).reshape((nfiles*ntsmp))\n",
    "\n",
    "\n",
    "# Reshape (concatenate the files for each minute into 10 mintes of coninous data)\n",
    "s_rdata = None # the buffer is still in global memory\n",
    "s_tdata = None # the buffer is still in global memory\n",
    "s_rdata = np.frombuffer(buf_rdata,dtype=np.float32).reshape((nchan, nfiles*ntsmp))\n",
    "s_tdata = np.frombuffer(buf_tdata,dtype=np.int64).reshape((nfiles*ntsmp))\n",
    "\n",
    "\n",
    "# To make it easy to follow, we switch to these variable names (only once is out_tr needed)\n",
    "raw_data = s_rdata.copy()\n",
    "in_tr = s_rdata\n",
    "out_tr = cc_rdata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015d677e",
   "metadata": {},
   "source": [
    "## Begin Processing: Function Defs\n",
    "\n",
    "**Need to make significant changes to these functions. No returns; start and end indices; global/shared memory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13693b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No JIT'ing!\n",
    "#@numba.njit(cache=True, fastmath=True, nogil=True, parallel=True)\n",
    "def remove_median_xchannel(start,end):\n",
    "    global in_tr #shared memory\n",
    "    for i in range(start,end):\n",
    "        in_tr[:, i] -= np.median(in_tr[:, i])\n",
    "\n",
    "\n",
    "# CANNOT jit ANY of the functions below\n",
    "\n",
    "def detrend_all_traces(start,end):\n",
    "    global in_tr #shared memory\n",
    "    signal.detrend(in_tr[start:end],type='constant',overwrite_data=True)\n",
    "    signal.detrend(in_tr[start:end],type='linear',overwrite_data=True)\n",
    "        \n",
    "\n",
    "def bandpass_butter_all_traces(start, end, fs=None, b0=None, bN=None, order=5):\n",
    "    global in_tr #shared memory\n",
    "    global out_tr #shared memory\n",
    "    sos = signal.butter(order, (b0,bN), 'bandpass', fs=fs, output='sos')\n",
    "    out_tr[start:end,:] = signal.sosfiltfilt(sos, in_tr[start:end])[:]\n",
    "\n",
    "\n",
    "#FIXME: maybe this is why there is a difference between all?\n",
    "def silly_zero_data(d2z,i):\n",
    "    d2z[i] = 0\n",
    "    \n",
    "\n",
    "# swap views for global/shared memory arraay\n",
    "def swap_views():\n",
    "    global in_tr\n",
    "    global out_tr\n",
    "    tmp = in_tr\n",
    "    in_tr = out_tr\n",
    "    out_tr = tmp\n",
    "    tmp = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e727cfb2",
   "metadata": {},
   "source": [
    "## Scale-up Cluster for Data Processing: (All Cores)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d6132667",
   "metadata": {},
   "source": [
    "%%time\n",
    "client.cluster.scale(ncore)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f7b625",
   "metadata": {},
   "source": [
    "## Scatter Concatenated Data to All Cores (from Notebook to Cluster)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4f4cc37e",
   "metadata": {},
   "source": [
    "%%time\n",
    "future = client.scatter(list(rdata))\n",
    "junk = wait(future)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35859031",
   "metadata": {},
   "source": [
    "## Detrend Data Per Channel: Multiple Channels Per Thread (Scheduler Decides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87974912",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "# We want a total of \"ncore\" chuncks (range of channels)\n",
    "chan_chnk = nchan//ncore\n",
    "\n",
    "# start and end indices into shared memory\n",
    "slst = [i for i in range(0,nchan,chan_chnk)]\n",
    "elst = [i for i in range(chan_chnk,nchan+1,chan_chnk)]\n",
    "pargs = pargs = list(zip(slst,elst))\n",
    "\n",
    "\n",
    "# Parallel part\n",
    "with multiprocess.Pool(processes=ncore) as pool:\n",
    "    pool.starmap(detrend_all_traces, pargs)\n",
    "    # output is stored in in_tr shared-array\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc12b3d",
   "metadata": {},
   "source": [
    "## Bandpass Filter Per Channel: (Same as for Detrend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44105b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Bandpass parameters\n",
    "bl = 0.025\n",
    "#br = 5.0\n",
    "#bl = 0.075\n",
    "br = 1.5\n",
    "fs = 200\n",
    "\n",
    "# start and end indices into shared memory\n",
    "bl_lst = [bl for i in range(ncore)]\n",
    "br_lst = [br for i in range(ncore)]\n",
    "fs_lst = [fs for i in range(ncore)]\n",
    "pargs = pargs = list(zip(slst,elst,fs_lst,bl_lst,br_lst))\n",
    "\n",
    "\n",
    "# Parallel part\n",
    "with multiprocess.Pool(processes=ncore) as pool:\n",
    "    pool.starmap(bandpass_butter_all_traces, pargs)\n",
    "    # output is stored in out_tr shared-array\n",
    "    \n",
    "    \n",
    "swap_views() # like pointer swapping (for shared arrays)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ee115f",
   "metadata": {},
   "source": [
    "## Decimate Per Channel: (Slightly Faster than Serial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938a42e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "ss = 30\n",
    "ohz = 200\n",
    "in_tr = in_tr[:,::ss] # easy peasy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c362aef",
   "metadata": {},
   "source": [
    "## Gather Processed Data: (From cluster to Notebook)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "eabb750d",
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "dec_bp_data = np.asarray(client.gather(bp_data,direct=True),dtype=rdata_dtype)\n",
    "#                          ^               #NOTE: ------------^\n",
    "#                          |\n",
    "# --- LOOK ----------------  # for some reason this is slightly faster than two lines of code\n",
    "\n",
    "\n",
    "# NOTE: np.vstack() has ~same RUNTIME as np.asarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c660be",
   "metadata": {},
   "source": [
    "## Release Cluster and Scheduler: (and All Related Resources, i.e. Memory, Cores, etc.)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "74f04418",
   "metadata": {},
   "source": [
    "%%time\n",
    "client.shutdown()\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4c8fa3",
   "metadata": {},
   "source": [
    "## X-Channel Median Removal Per Time-Sample: (Serial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2780a17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "# concatenated and decimated num time samps\n",
    "# again, find chunck size for \"ncore\" chunks (but in time)\n",
    "dntsmp = (ntsmp*nfiles)//ss \n",
    "time_chnk = dntsmp//ncore\n",
    "\n",
    "# start and end indices into shared memory\n",
    "slst = [i for i in range(0,dntsmp,time_chnk)]\n",
    "elst = [i for i in range(time_chnk,dntsmp+1,time_chnk)]\n",
    "pargs = pargs = list(zip(slst,elst))\n",
    "\n",
    "\n",
    "# Parallel part\n",
    "with multiprocess.Pool(processes=ncore) as pool:\n",
    "    pool.starmap(remove_median_xchannel, pargs)\n",
    "    # output is stored in in_tr shared-array\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4223d803",
   "metadata": {},
   "source": [
    "## Total Processing Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff88068",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Done Processing')\n",
    "runtime = time() - start_time #start is in first cell\n",
    "print(f'runtime: {datetime.timedelta(seconds=runtime)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbfc826",
   "metadata": {},
   "source": [
    "## Define function for plotting: (Serial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5d9ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_seven_trees_data(data,times,pclip=.95,fig_size=(9,10)):\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    eqdate = datetime.datetime.utcfromtimestamp(times[0]//1000000)\n",
    "    start_c = 22_000\n",
    "    end_c = 28_000\n",
    "    #start_c = 23_225 # start spool?\n",
    "    #end_c = 23_300   # end   spool?\n",
    "    bounds = (0,nfiles*60,end_c,start_c)\n",
    "\n",
    "\n",
    "    vclip = (1-pclip)*np.abs(data[start_c:end_c+1,:]).max()\n",
    "\n",
    "\n",
    "    plt.figure(figsize=fig_size)\n",
    "    plt.imshow(data[start_c:end_c+1,:], aspect='auto', interpolation='none', cmap='gray', vmin=-vclip, vmax=vclip, extent=bounds)\n",
    "    #plt.title('DAS for Seven Trees 1st-Aftershock, 3.1 EQ @' + str(eqdate) )\n",
    "    #plt.title('DAS Data: ' + str(eqdate))\n",
    "    plt.title('Recorded: ' + str(eqdate))\n",
    "    plt.xlabel('seconds from: ' + str(eqdate.time()))\n",
    "    plt.ylabel('channel')\n",
    "    \n",
    "    outfig = plt.gcf()\n",
    "    \n",
    "    return plt,outfig,eqdate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285e0ed0",
   "metadata": {},
   "source": [
    "## 2D Plot of the Processed DAS Data: (Serial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a594f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "#%%time\n",
    "\n",
    "pclip = .99\n",
    "#pclip = .0\n",
    "print(f'pclip: {pclip}')\n",
    "\n",
    "plt, fig, eqdate = plot_seven_trees_data(in_tr,s_tdata,pclip=pclip)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c4075b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
